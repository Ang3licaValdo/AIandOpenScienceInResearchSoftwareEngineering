<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">State of the Art: Reproducibility in Artificial Intelligence</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Erik</forename><surname>Odd</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Norwegian University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sigbjørn</forename><surname>Gundersen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Norwegian University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><surname>Kjensmo</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Norwegian University of Science and Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">State of the Art: Reproducibility in Artificial Intelligence</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">586848945A305DB4A79686B6EF0BCA11</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3-SNAPSHOT" ident="GROBID" when="2023-02-09T00:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Background: Research results in artificial intelligence (AI) are criticized for not being reproducible. Objective: To quantify the state of reproducibility of empirical AI research using six reproducibility metrics measuring three different degrees of reproducibility. Hypotheses: 1) AI research is not documented well enough to reproduce the reported results. 2) Documentation practices have improved over time. Method: The literature is reviewed and a set of variables that should be documented to enable reproducibility are grouped into three factors: Experiment, Data and Method. The metrics describe how well the factors have been documented for a paper. A total of 400 research papers from the conference series IJCAI and AAAI have been surveyed using the metrics. Findings: None of the papers document all of the variables. The metrics show that between 20% and 30% of the variables for each factor are documented. One of the metrics show statistically significant increase over time while the others show no change. Interpretation: The reproducibility scores decrease with increased documentation requirements. Improvement over time is found. Conclusion: Both hypotheses are supported.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Although reproducibility is a cornerstone of science, a large amount of published research results cannot be reproduced. This is even the case for results published in the most prestigious journals; even the original researchers cannot reproduce their own results <ref type="bibr" target="#b0">(Aarts et al. 2016;</ref><ref type="bibr" target="#b1">Begley and Ellis 2012;</ref><ref type="bibr" target="#b2">Begley and Ioannidis 2014;</ref><ref type="bibr" target="#b20">Prinz, Schlange, and Asadullah 2011)</ref>. <ref type="bibr" target="#b13">(Goodman, Fanelli, and Ioannidis 2016)</ref> presents data from Scopus that shows that the problem with reproducibility spans several scientific fields. According to <ref type="bibr" target="#b10">(Donoho et al. 2009</ref>) "it is impossible to verify most of the computational results presented at conferences and in papers today". This was confirmed by <ref type="bibr" target="#b8">(Collberg and Proebsting 2016)</ref>. Out of 402 experimental papers they were able to repeat 32.3% without communicating with the author, rising to 48.3% with communication. Papers by authors with industry affiliation showed a lower rate of reproducibility. They also found that some researchers are not willing to share code and data, while those that actually share, provide too little to repeat the experiment.</p><p>Guidelines, best-practices and solutions to aid reproducibility point towards open data and open source code as requirements for reproducible research <ref type="bibr" target="#b21">(Sandve et al. 2013;</ref><ref type="bibr" target="#b21">Stodden and Miguez 2014)</ref>. The increased focus on reproducibility has resulted in an increased adoption of data and code sharing policies for journals <ref type="bibr" target="#b22">(Stodden, Guo, and Ma 2013)</ref>. Still, proposed solutions for facilitating reproducibility see little adoption due to low ease-of-use and the time required to retroactively fit an experiment to these solutions <ref type="bibr" target="#b13">(Gent and Kotthoff 2014)</ref>. <ref type="bibr" target="#b4">(Braun and Ong 2014)</ref> argues that automation should be possible to a higher degree for machine learning, as everything needed is available on a computer. Despite of this, the percentage of research that is reproducible is not higher for machine learning and artificial intelligence (AI) research <ref type="bibr" target="#b15">(Hunold and Träff 2013;</ref><ref type="bibr" target="#b12">Fokkens et al. 2013;</ref><ref type="bibr" target="#b16">Hunold 2015)</ref>.</p><p>The scientific method is based on reproducibility; "if other researchers can't repeat an experiment and get the same result as the original researchers, then they refute the hypothesis" <ref type="bibr">(Oates 2006, p. 285)</ref>. Hence, the inability to reproduce results affects the trustworthiness of science. To ensure high trustworthiness of AI and machine learning research measures must be taken to increase its reproducibility. However, before measures can be taken, the state of reproducibility in AI research must be documented. The state of reproducibility can only be documented if a proper framework is built.</p><p>Our objective is to quantify the state of reproducibility of empirical AI research, and our main hypothesis is that the documentation of AI research is not good enough to reproduce the reported results. We also investigate a second hypothesis, which is that documentation practices have improved during recent years. Two predictions were made, one for each hypothesis. The first prediction is that the current documentation practices at top AI conferences render most of the reported research results irreproducible, and the second prediction is that a larger portion of the reported research results are reproducible when comparing the latest installments of conferences to earlier installments. We surveyed research papers from the two top AI conference series, International Joint Conference on AI (IJCAI) and the Association for the Advancement of AI (AAAI) to test the hypotheses. Our contributions are twofold: i) an investigation of what reproducibility means for AI research and ii) a quantification of the state of reproducibility of AI research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reproducing Results</head><p>We base the survey on a concise definition of reproducibility and three degrees of reproducibility. These definitions are based a review of the scientific method and the literature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The Scientific Method in AI Research</head><p>Different strategies for researching information systems and computing exist <ref type="bibr">(Oates 2006)</ref>, and these include theory development and experiments among others. The scientific method and reproducibility is closely connected to experiments and empirical studies. We can distinguish between four different classes of empirical studies: 1) exploratory, 2) assessment, 3) manipulation and 4) observational studies <ref type="bibr" target="#b7">(Cohen 1995)</ref>. While exploratory and assessment studies are conducted to identify and suggest possible hypotheses, manipulation and observational studies test explicit and precise hypotheses. Although the scientific method is based on evaluating hypothesis, exploratory and assessment studies are not mandatory sub-processes of it. However, they may be conducted in order to formulate the hypotheses.</p><p>The targets of study in AI research are AI programs and their behavior <ref type="bibr" target="#b7">(Cohen 1995)</ref>. Changes to the AI program's structure, the task or the environment can affect the program's behavior. An AI program implements an abstract algorithm or system as a program that can be compiled and executed. Hence, the AI program is something distinct from the conceptual idea that it implements, which we will refer to as an AI method. Experiments should be formulated in such a way that it is clear whether they test hypotheses about the AI program or the AI method. Examples of tasks performed by AI methods include classification, planning, learning, decision making and ranking. The environment of the AI program is described by data. Typically, when performing AI experiments in supervised learning, the available data has to be divided into a training set, a validation set and a test set <ref type="bibr" target="#b20">(Russell and Norvig 2009)</ref>.</p><p>According to the scientific method and before performing an experiment, one should formulate one or more hypotheses about the AI program under investigation and make predictions about its behavior. The results of the experiments are interpreted by comparing their results to the hypotheses and the predictions. Beliefs about the AI program should be adjusted by this interpretation. The adjusted beliefs can be used to formulate new hypotheses, so that new experiments can be conducted. If executed honestly with earnest interpretations of the results, the scientific method updates our beliefs about an AI program so that they should converge towards objective truth. Figure <ref type="figure" target="#fig_0">1</ref> illustrates the scientific process of AI research as described here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The Terminology of Reproducibility</head><p>While researchers in computer science agree that empirical results should be reproducible, what is meant by reproducibility is neither clearly defined nor agreed upon. <ref type="bibr" target="#b23">(Stodden 2011)</ref> distinguishes between replication and reproduction. Replication is seen as re-running the experiment with code and data provided by the author, while reproduction is a broader term "implying both replication and the regeneration of findings with at least some independence from the <ref type="bibr">[original]</ref> code and/or data". <ref type="bibr" target="#b11">(Drummond 2009)</ref> states that replication, as the weakest form of reproducibility, can only achieve checks for fraud. Due to the inconsistencies in the use of the terms replicability and reproducibility, <ref type="bibr" target="#b13">(Goodman, Fanelli, and Ioannidis 2016)</ref> proposes to extend reproducibility into: Methods reproducibility: The ability to implement, as exactly as possible, the experimental and computational procedures, with the same data and tools, to obtain the same results. Results reproducibility: The production of corroborating results in a new study, having used the same experimental methods. Inferential reproducibility: The drawing of qualitatively similar conclusions from either an independent replication of a study or a reanalysis of the original study. Replication, as used by <ref type="bibr" target="#b11">(Drummond 2009)</ref> and <ref type="bibr" target="#b23">(Stodden 2011)</ref>, is in line with methods reproducibility as proposed by (Goodman, Fanelli, and Ioannidis 2016) while reproducibility seems to entail both results reproducibility and inferential reproducibility. <ref type="bibr" target="#b19">(Peng 2011)</ref> on the other hand suggests that reproducibility is on a spectrum from publication to full replication. This view neglects that results produced by AI methods can be reproduced using different data or different implementations. Results generated by using other implementations or other data can lead to new interpretations, which broadens the beliefs about the AI method, so that generalizations can be made. Despite the disagreements in terminology, there is a clear agreement on the fact that the reproducibility of research results is not one thing, but that empirical research can be assigned to some sort of spectrum, scale or ranking that is decided based on the level of documentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reproducibility</head><p>We define reproducibility in the following way: Definition. Reproducibility in empirical AI research is the ability of an independent research team to produce the same results using the same AI method based on the documentation made by the original research team.</p><p>Hence, reproducible research is empirical research that is documented in such detail by a research team that other researchers can produce the same results using the same AI method. According to <ref type="bibr" target="#b21">(Sandve et al. 2013</ref>), a minimal requirement of reproducibility is that you should at least be able to reproduce the results yourself. We interpret this as repeatability and not reproducibility. Our view is that an important aspect of reproducibility is that the experiment is conducted independently. We will briefly discuss the three terms AI method, results and independent research team in this section. The next section is devoted to documentation.</p><p>An independent research team is one that conducts the experiment by only using the documentation made by the original research team.Enabling others to reproduce the same results is closely related to trust. Most importantly, other researchers can be expected to be more objective. They have no interest in inflating the performance of a method they have not developed themselves. More practically, they will not share the same preconceptions and implicit knowledge as the first team reporting the research. Also, other researchers will not share the exact same hardware running the exact same copies of software. All of this helps controlling for noise variables related to both the hardware and ancillary software as well as implicit knowledge and preconceptions.</p><p>The distinction between the AI program and the AI method is important. We must as far as possible remove any uncertainties to whether other effects than the AI method are responsible for the results. The concept of using an agent system for solving some problem is different from the specific implementation of the agent system. If the results are dependent on the implementation of the method, the hardware it is running on or the experiment setup, then the characteristics of the AI method do not cause the results.</p><p>The results are the output of the experiment, in other words, the dependent variables of the experiment <ref type="bibr" target="#b7">(Cohen 1995)</ref>, which typically are captured by performance measures. The result is the target of the investigation when reproducing an experiment; we want to ensure that the performance of the AI method is the same even if we change the implementation, the operating system or the hardware that is being used to conduct the experiment. As long as the results of the original and the reproduced experiments are the same, the original experiment is reproducible. What constitutes the same results depends on to which degree the results are reproduced.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Documenting for Reproducibility</head><p>In order to reproduce the results of an experiment, the documentation must include relevant information, which must be specified to a certain level of detail. What is relevant and how detailed the documentation must be are guided by whether it is possible to reproduce the results of the experiment using this information only. Hence, the color of the researcher's jacket is usually not relevant for reproducing the results. Which operating system is used on the machine when executing the experiment can very well be relevant though.</p><p>So what exactly is relevant information? The objective of <ref type="bibr" target="#b6">(Claerbout and Karrenbach 1992;</ref><ref type="bibr" target="#b5">Buckheit and Donoho 1995)</ref> was to make it easy to rerun experiments and trace methods that produced the reported results. For <ref type="bibr" target="#b6">(Claerbout and Karrenbach 1992)</ref>, this meant sharing everything on a CD-ROM, so that anyone could read the research report and execute the experiments by pushing a button attached to every figure. <ref type="bibr" target="#b5">(Buckheit and Donoho 1995)</ref> shared Wavelab, a Matlab package, that made all the code needed for reproducing their figures in one of their papers. (Goodman, Fanelli, and Ioannidis 2016) highlights that "reporting of all relevant aspects of scientific design, conduct, measurements, data and analysis" is necessary for all three types of reproducibility. This is in line with the view of <ref type="bibr" target="#b23">(Stodden 2011)</ref>, which is that availability of the computational environment is necessary for computational reproducibility. <ref type="bibr" target="#b19">(Peng 2011)</ref> argues that a paper alone is not enough, but that linked and executable code and data is the gold standard. We have grouped the documentation into three categories: 1) method, 2) data and 3) experiment.</p><p>Method: The method documentation includes the AI method that the AI program implements as well as the a motivation of why the method is used. As the implementation does not contain the motivation and intended behavior, sharing the implementation of the AI program is not enough. It is important to give a high-level description of the AI method that is being tested. This includes what the AI method intends to do, why it is needed and how it works. To decrease ambiguity, a description of how a method works should contain pseudo code and an explanation of the pseudo code containing descriptions of the parameters and sub-procedures. Sharing of the AI method is the objective of most research papers in AI. The problem that is investigated must be specified, the objective of the research must be clear and so must the research method being used.</p><p>Data: Sharing the data used in the experiment is getting simpler with open data repositories, such as the UCI Machine Learning Repository <ref type="bibr" target="#b18">(Lichman 2013)</ref>. Reproducing the results fully requires the procedure for how the data set has been divided into training, validation and test sets and which samples belong to the different sets. Sharing the validation set might not be necessary when all samples in the training set are used or might be hard when the method picks the samples randomly during the experiment. Data sets often change, so specifying the version is relevant. Finally in order to compare results, the actual output of the experiment, such as the classes or decisions made, are required.</p><p>Experiment: For others to reproduce the results of an experiment, the experiment and its setup must be shared. The experiment contains code as well as an experiment description. Proper experiment documentation must explain the purpose of the experiment. The hypotheses that are tested and the predictions about these must be documented, and so must the results and the analysis. In order to rule out the possibilities that the results can be attributed to the hardware or ancillary software, the hardware and ancillary software used must be properly specified. The ancillary software includes, but is not restricted to, the operating system, programming environment and programming libraries used for implementing the experiment. Sharing the experiment code is not limited to open sourcing the AI program that is investigated, but sharing of the experiment setup with all independent variables, such as hyperparameters, as well as the scripts and environmental settings is required. The experiment setup consists of independent variables that control the experiment. These variables configure both the ancillary software and the AI program. Hyperparameters are independent variables that configure the AI method and examples include the number of leaves or depth of a tree and the learning rate. Documented code increases transparency.</p><p>In conclusion, there are different degrees to how well an empirical study in AI research can be documented. The degrees depend on whether the method, the data and the experiment are documented and how well they are documented. The gold standard is sharing all of the three groups of documentation through access to a running virtual machine in the cloud containing all the data, runnables, documentation and source code, as this includes the hardware and software stack as well and not only the software libraries used for running the experiments which was the case with the proposed solutions by <ref type="bibr" target="#b6">(Claerbout and Karrenbach 1992;</ref><ref type="bibr" target="#b5">Buckheit and Donoho 1995)</ref>. This is not necessarily practical, as it requires costly infrastructure that has a high maintenance cost. Another practical consideration is related to how long the infrastructure should and can be guaranteed to run and produce the same results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Degrees of Reproducibility</head><p>We propose to distinguish between three different degrees of reproducibility, where an increased degree of reproducibility conveys an increased generality of the AI method. An increased generality means that the performance of the AI method documented in the experiment is not related to one specific implementation or the data used in the experiment; the AI method is more general than that. The three degrees of reproducibility are defined as follows: R1: Experiment Reproducible The results of an experiment are experiment reproducible when the execution of the same implementation of an AI method produces the same results when executed on the same data. R2: Data Reproducible The results of an experiment are data reproducible when an experiment is conducted that executes an alternative implementation of the AI method that produces the same results when executed on the same data.</p><p>R3: Method Reproducible The results of an experiment are method reproducible when the execution of an alternative implementation of the AI method produces the same results when executed on different data. Results that are R1 reproducible require the same software and data used for conducting the experiment and a detailed description of the AI method and experiment. This is what is called fully reproducible by <ref type="bibr" target="#b19">(Peng 2011</ref>) and method reproducibility by (Goodman, Fanelli, and Ioannidis 2016). We call it experiment reproducible as everything required to run the experiment is needed to reproduce the results. The results when re-running the experiment should be exactly the same, as reported in the original experiment. Any differences can only be attributed to differences in hardware given that the ancillary software is the same.</p><p>Results that are R2 reproducible require only the method description and the data in order to be reproduced. This removes any noise variables related to implementation and hardware. The belief that the result is being caused by the AI method is strengthened. Hence, the generality of the AI method is increased compared to an AI method that is R1 reproducible. As the results are achieved by running the AI method on the same data as the original experiment, there is still a possibility that the performance can only be achieved using the same data. The results that are produced, the performance, using a different implementation should be the same if not exactly the same. Differences in results can be attributed to different implementations and hardware, such as different ways of doing floating point arithmetic. However, differences in software and hardware could have significant impact on results because of rounding errors in floating point arithmetic <ref type="bibr" target="#b14">(Hong et al. 2013)</ref>.</p><p>Results that are R3 reproducible only requires the method documentation to be reproduced. If the results are reproduced, all noise variables related to implementation, hardware and data have been removed, and it is safe to assume that the results are caused by the AI method. As the results are produced by using a new implementation on a new data set, the AI method is generalized to other data and the implementation used in the original experiment. In order for a result to be R3 reproducible the results of the experiments must support the same hypotheses and thus support the same beliefs. The same interpretations cannot be made unless the results are statistically significant, so the analysis should be supported by statistical hypothesis testing with a pvalue of 0.005 for claiming new discoveries <ref type="bibr" target="#b17">(Johnson 2013;</ref><ref type="bibr" target="#b3">Benjamin et al. 2017)</ref>.</p><p>When it comes to generality of the results the following is true: R1 &lt; R2 &lt; R3, which means that R1 reproducible results are less general than R2 reproducible results, which in turn are less general than R3 reproducible results. How-ever, when it comes to the documentation required, the following is the case: doc(R3) ⊂ doc(R)2 ⊂ doc(R1). The documentation needed for R3 reproducibility is a subset of the documentation required for R2 reproducibility and the documentation required for R2 is a subset of the documentation required for R1 reproducibility. R3 reproducible is the most general reproducibility degree that also requires the least amount of information.</p><p>Current practice of publishers is not to require researchers to share data and implementation when publishing research papers. The current practice enables R3 reproducible results that have the least amount of transparency. For (Goodman, Fanelli, and Ioannidis 2016), the goal of transparency is to ease evaluation of the weight of evidence from studies to facilitate future studies on actual knowledge gaps and cumulative knowledge, and reduce time spent exploring blind alleys from poorly reported research. This means that current practices enable other research teams to reproduce results at the highest reproducibility degree with the least effort of the original research team. The majority of effort in reproducing results, lays with the independent team, instead of the original team. Transparency does not only reduce the effort needed to reproduce the results, but it also builds trust in them. Hence, the results that are produced by current practices are the least trustworthy from a reproducibility point of view, because of the lack in transparency; the evidence showing that the results are valid is not published.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Research Method</head><p>We have conducted an observational experiment in form of a survey of research papers in order to generate quantitative data about the state of reproducibility of research results in AI. The research papers have been reviewed, and a set of variables have been manually registered. In order to compare results between papers and conferences, we propose six metrics for deciding whether research results are R1, R2, and R3 reproducible as well as to which degree they are.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Survey</head><p>In order to evaluate the two hypotheses, we have surveyed a total of 400 papers where 100 papers have been selected from each of the 2013 and 2016 installments of the conference IJCAI and from the 2014 and 2016 installments of the conference series AAAI. With an exception of 50 papers from IJCAI 2013, all the papers have been selected randomly to avoid any selection biases. Table <ref type="table" target="#tab_0">1</ref> shows the number of accepted papers (the population size), the number of surveyed papers (sample size) and the margin of errors for a confidence level of 95% for the four conferences. We have computed the margin of error as half the width of the confidence interval, and for our study the margin of error is 4.29%. All the data and the code that has been used to calculate the reproducibility scores and generate the figures can be found on Github<ref type="foot" target="#foot_1">1</ref> .  All variables were registered as true (1) or false (0) unless otherwise specified. When surveying the papers, we have looked for explicit mentions of the variables marked with an asterix (*) above. For example, when reviewing the variable Problem, we have looked for an explicit mention of the problem being solved, such as "To address this problem, we propose a novel navigation system ..." <ref type="bibr" target="#b9">(De Weerdt et al. 2013)</ref>.</p><p>The decision to use explicit mentions of the terms, such as contribution, goal, hypothesis and so on, can be disputed. However, the reasons for looking for explicit mentions are both practical and idealistic. Practically, it is easier to review a substantial amount of papers if the criteria are clear and objective. If we did not follow this guideline, the registering of variables would lend itself to subjective assessment rather than objective, and the results could be disputed based on how we measured the variables. Our goal was to get results with a low margin of error, so that we could draw statistically valid conclusions. In order to survey enough papers, we had to reduce the time we used on each paper. Explicit mentions supported this. Idealistically, our attitude is that research documentation should be clear and concise. Explicit mentions of which problem is being solved, what the goal of doing the research is, which hypothesis is being tested and so on are required to remove ambiguity from the text. Less ambiguous documentation increases the reproducibility of the research results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Quantifying Reproducibility</head><p>We have defined a set of six metrics to quantify whether an experiment e is R1, R2 or R3 reproducible and to which degree. The metrics measure how well the three factors method, data and experiment are documented. The three metrics R1(e), R2(e) and R3(e) are boolean metrics that can be either true or false: </p><p>where Method(e), Data(e) and Exp(e) is the weighted sum of the truth values of the variables listed under the three factors Method, Data and Experiment. The weights of the factors are δ 1 , δ 2 and δ 3 respectively. This means that the value for Data(e) for experiment e is the summation of the truth values for whether the training, validation, and test data sets as well as the results are shared for e. It is of course also possible to give different weights to each variable of a factor. We use a uniform weight for all variables and factors for our survey, δ i = 1. For an experiment e 1 that has published the training data and test data, but not the validation set and the results Data(e) = 0.5. Note that some papers have no value for the training and validation sets if the experiment does not require either. For these papers, the δ i weight is set to 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results and Discussion</head><p>Figure <ref type="figure" target="#fig_2">3</ref> shows percentage of research papers that have documented the different variables for the three factors. None of the three factors are documented very well according to the The variable Experiment setup is given a high score, which indicates that the experiment setup is documented to some degree. As we have not actually tried to reproduce the results, we have not ensured that the experiment setup is documented in enough detail to run the experiment. The amount of empirical papers are shown in Table <ref type="table" target="#tab_0">1</ref>. For each conference, between 15% and 29% of the the randomly selected samples are not empirical. In total, 325 papers empirical and considered in the analysis. Table <ref type="table">2</ref> presents the results of the RXD (R1D, R2D and R3D) metrics. All the RXD metrics vary between 0.20 and 0.30. This means that only between a fifth and a third of the variables required for reproducibility are documented. For all papers, R1D has the lowest score with 0.24, R2D has a score of 0.25 and R3D has a score of 0.26. The general trend is that R1D is lower than the R2D scores, which again are lower than the R3D scores. This is not surprising, as R1D has fewer variables than R2D, which has fewer variables than R3D. However, given the error there is little variation among the three reproducibility degrees.</p><p>The RX (R1, R2 and R3) scores were 0.00 for all papers. No paper had full score on all variables for the factor Method, and it is required for all the three RX metrics. The three RX metrics are very strict and are not very informative for a survey such as this. They might have a use though, as guidelines for reviewers of conferences and journal publications. The three RXD metrics do not have the same issue as the RX metrics, as they measure the degree of reproducibility between 0 and 1.</p><p>There is a clear increase in the RXD scores from IJCAI 2013 to IJCAI 2016, see figure <ref type="figure" target="#fig_4">4 a</ref>). However, the trend is not as clear for AAAI as the R2D and R3D scores decrease. Table <ref type="table" target="#tab_1">3</ref>   and 2014, 156 papers) and the combined scores for 2016 (169 papers). The results show that there is a slight, but statistically significant increase for R1D. The increase for R2D is not statistically significant, and there is no change for R3D. This means that only the experiment documentation has improved with time, and that there is no such evidence for the documentation of methods and data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>The survey confirms our prediction that the current documentation practices at top AI conferences render most of the reported research results irreproducible, as the R1, R2 and R3 reproducibility metrics show that no papers are fully reproducible. Only 24% of the variables required for R1D reproducibility, 25% of the variables required for R2D reproducibility and 26% of the variables required for R3D reproducibility are documented. When investigating whether there is change over time, we see improvement, which then confirms our second hypothesis. No improvement is indicated by the R1, R2, R3, R2D and R3D metrics. There is however a statistically significant improvement in the R1D metric. Hence, overall there is an improvement.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: By comparing the results of an experiment to the hypotheses and predictions that are being made about the AI program, we interpret the results and adjust our beliefs about them.</figDesc><graphic coords="3,134.94,53.71,342.13,79.42" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The three degrees of reproducibility are defined by which documentation is used to reproduce the results.</figDesc><graphic coords="4,324.54,53.77,228.43,69.64" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Percentage of papers documenting each variable for the three factors: a) Method, b) Data and c) Experiment.</figDesc><graphic coords="6,56.52,53.71,499.00,116.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>R1(e) = Method(e) ∧ Data(e) ∧ Exp(e),Method(e),Data(e)  and Exp(e) is the conjunction of the truth values of the variables listed under the three factors Method, Data and Experiment in the section Factors and Variables. This means that for Data(e) to be true for an experiment e, the training data set, the validation data set, the test data set and the results must be shared for e. Hence, R1(e) is the most strict requirement while R3 is the most relaxed requirement when it comes to the documentation of an experiment e, as R3(e) requires only variables of the factor Method to be true while R1(e) requires all variables for all the three factors to be true.The three metrics R1(e), R2(e) and R3(e) are boolean metrics, so they will provide information on whether an experiment is R1, R2 or R3 reproducible in a strict sense. They will however not provide any information on to which degree experiments are reproducible, unless an experiment meets all the requirements. Therefore we suggest the three metrics R1D(e), R2D(e) and R3D(e) for measuring to which degree the the results of an experiment e is: R1D(e) = δ 1 Method(e) + δ 2 Data(e) + δ 3 Exp(e) δ 1 + δ 2 + δ 3 (4) R2D(e) = δ 1 Method(e) + δ 2 Data(e) δ 1 + δ 2 , (5) R3D(e) = Method(e),</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: a) Change in the RXD metrics. b), c) and d) show the amount of variables registered for the three factors for all papers.</figDesc><graphic coords="7,56.34,53.71,499.33,117.85" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>shows the combined scores for the earlier years (2013 Table 2: The 95% confidence interval for the mean R1D, R2D and R3D scores where ε = 1.96σ x and σ x = σ √ N . Conference R1D ± ε R2D ± ε R3D ± ε IJCAI 2013 0.20 ± 0.02 0.20 ± 0.03 0.24 ± 0.04 AAAI 2014 0.21 ± 0.02 0.26 ± 0.03 0.28 ± 0.04 IJCAI 2016 0.30 ± 0.03 0.30 ± 0.04 0.29 ± 0.04 AAAI 2016 0.23 ± 0.02 0.25 ± 0.04 0.24 ± 0.04 Total 0.24 ± 0.01 0.25 ± 0.02 0.26 ± 0.02</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Population size, sample size (with number of empirical studies) and margin of error for a confidence level of 95% for the four conferences and total population.</figDesc><table><row><cell cols="3">Conference Population size Sample size</cell><cell>MoE</cell></row><row><cell>IJCAI 2013</cell><cell>413</cell><cell>100 (71)</cell><cell>8.54%</cell></row><row><cell>AAAI 2014</cell><cell>213</cell><cell>100 (85)</cell><cell>7.15%</cell></row><row><cell>IJCAI 2016</cell><cell>551</cell><cell>100 (84)</cell><cell>8.87%</cell></row><row><cell>AAAI 2016</cell><cell>549</cell><cell>100 (85)</cell><cell>8.87 %</cell></row><row><cell>Total</cell><cell>1726</cell><cell>400 (325)</cell><cell>4.30%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 :</head><label>3</label><figDesc>The 95% confidence interval for the mean R1D, R2D and R3D scores when combining the papers from all four installments of IJCAI and AAAI into two groups according to the years they were published. One group contains all papers from 2013 and 2014 and the other group contains all the papers from 2016.</figDesc><table><row><cell>Years</cell><cell>R1D ± ε</cell><cell>R2D ± ε</cell><cell>R3D ± ε</cell></row><row><cell cols="4">2013/2014 0.21 ± 0.02 0.23 ± 0.02 0.26 ± 0.03</cell></row><row><cell>2016</cell><cell cols="3">0.27 ± 0.02 0.27 ± 0.03 0.26 ± 0.03</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0">The Thirty-Second AAAI Conference on Artificial Intelligence </note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_1">https://github.com/aaai2018-paperid-62/aaai2018-paperid-62</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work has been carried out at the Telenor-NTNU AI Lab, Norwegian University of Science and Technology, Trondheim, Norway.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Factors and Variables</head><p>We have identified a set of variables that we believe are good indicators for reproducibility after reviewing the literature. These variables have been grouped together into the three factors Method, Data and Experiment. For each surveyed paper, we have registered these variables. In addition, we have collected some extra variables, which have been grouped together in Miscellaneous. The following variables have been registered for the three factors: Miscellaneous: Different variables that describe the research.</p><p>Research type: Experimental (E) or theoretical (T).</p><p>Research outcome: Is the paper reporting a positive or a negative result (positive=1 and negative=0). Affiliation: The affiliation of the authors. Academia (0), collaboration (1) or industry (2). Contribution (*): Contribution of the research.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Aarts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A L M</forename><surname>Van Assen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">R</forename><surname>Attridge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Attwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Axt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Babel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Bahník</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Baranski</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>Psychology</publisher>
		</imprint>
	</monogr>
	<note>Reproducibility project</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Drug development: Raise standards for preclinical cancer research</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">G</forename><surname>Begley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Ellis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">483</biblScope>
			<biblScope unit="issue">7391</biblScope>
			<biblScope unit="page" from="531" to="533" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Reproducibility in science: Improving the standard for basic and preclinical research</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">G</forename><surname>Begley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P A</forename><surname>Ioannidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Circulation Research</title>
		<imprint>
			<biblScope unit="volume">116</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="116" to="126" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Redefine statistical significance</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Benjamin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">O</forename><surname>Berger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Johannesson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Nosek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E.-J</forename><surname>Wagenmakers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Berk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Bollen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Brembs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Camerer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Human Behaviour</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Open science in machine learning</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Braun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Ong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Implementing Reproducible Research</title>
		<imprint>
			<biblScope unit="volume">343</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Wavelab and reproducible research</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Buckheit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Donoho</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995</date>
			<pubPlace>Standford, CA</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Electronic documents give reproducible research a new meaning</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Claerbout</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Karrenbach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 62nd Annual International Meeting of the Society of Exploration Geophysics</title>
				<meeting>the 62nd Annual International Meeting of the Society of Exploration Geophysics</meeting>
		<imprint>
			<date type="published" when="1992-10-29">1992. 25 to 29 October 1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Empirical methods for artificial intelligence</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">R</forename><surname>Cohen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995</date>
			<publisher>MIT press</publisher>
			<biblScope unit="volume">139</biblScope>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Repeatability in computer systems research</title>
		<author>
			<persName><forename type="first">C</forename><surname>Collberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">A</forename><surname>Proebsting</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="62" to="69" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Intention-aware routing to minimise delays at electric vehicle charging stations</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>De Weerdt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">H</forename><surname>Gerding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Robu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">R</forename><surname>Jennings</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Third international joint conference on Artificial Intelligence</title>
				<meeting>the Twenty-Third international joint conference on Artificial Intelligence</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="83" to="89" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Reproducible research in computational harmonic analysis</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Donoho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Maleki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">U</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shahram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Stodden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computing in Science &amp; Engineering</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Drummond</surname></persName>
		</author>
		<title level="m">Replicability is not reproducibility: nor is it good science. International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Offspring from reproduction problems: What replication failure teaches us</title>
		<author>
			<persName><forename type="first">A</forename><surname>Fokkens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">V</forename><surname>Erp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Postma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pedersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vossen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Freire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 51st Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1691" to="1701" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Recomputation. org: Experiences of its first year and lessons learned</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">P</forename><surname>Gent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kotthoff</surname></persName>
		</author>
		<author>
			<persName><surname>Ieee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">N</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fanelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P A</forename><surname>Ioannidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Utility and Cloud Computing (UCC)</title>
				<imprint>
			<date type="published" when="2014">2014. 2014. 2016</date>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="341" to="353" />
		</imprint>
	</monogr>
	<note>Science Translational Medicine</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">An evaluation of the software system dependency of a global atmospheric model</title>
		<author>
			<persName><forename type="first">S.-Y</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-S</forename><surname>Koo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-E</forename><forename type="middle">E</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-S</forename><surname>Joh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-H</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-J</forename><surname>Oh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Monthly Weather Review</title>
		<imprint>
			<biblScope unit="volume">141</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="4165" to="4172" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">On the state and importance of reproducible experimental research in parallel computing</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hunold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Träff</surname></persName>
		</author>
		<idno>CoRR, abs/1308.3648</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Hunold</surname></persName>
		</author>
		<idno>CoRR, abs/1511.04217</idno>
		<title level="m">A survey on reproducibility in parallel computing</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Revised standards for statistical evidence</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">E</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="issue">48</biblScope>
			<biblScope unit="page" from="19313" to="19317" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Researching Information Systems and Computing</title>
		<author>
			<persName><forename type="first">M</forename><surname>Lichman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2013. 2006</date>
			<publisher>SAGE Publications Ltd</publisher>
			<pubPlace>Oates, B. J.</pubPlace>
		</imprint>
	</monogr>
	<note>UCI machine learning repository</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Reproducible research in computational science</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">334</biblScope>
			<biblScope unit="issue">6060</biblScope>
			<biblScope unit="page" from="1226" to="1227" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Believe it or not: how much can we rely on published data on potential drug targets?</title>
		<author>
			<persName><forename type="first">F</forename><surname>Prinz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Schlange</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Asadullah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Norvig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Intelligence: A modern approach</title>
				<imprint>
			<publisher>Prentice-Hall</publisher>
			<date type="published" when="2009">2011. 2009</date>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="712" to="712" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Best practices for computational science: Software infrastructure and environments for reproducible and extensible research</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">K</forename><surname>Sandve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nekrutenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hovig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">C</forename><surname>Stodden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Miguez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Open Research Software</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">e21</biblScope>
			<date type="published" when="2013">2013. 2014</date>
		</imprint>
	</monogr>
	<note>PLoS Computational Biology</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Toward reproducible computational research: An empirical analysis of data and code policy adoption by journals</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">C</forename><surname>Stodden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS ONE</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">e67111</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Trust your science? Open your data and code</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">C</forename><surname>Stodden</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
	<note>Amstat News 21-22</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
